---
{"dg-publish":true,"permalink":"/philip-morgan-research-notes/ssr-guide-public-preview/","dgHomeLink":true,"dgPassFrontmatter":false}
---


This is a public preview of _The Small-Scale Research Guide_. Currently in-progress. What's complete enough to read:

- [1: The Potential Of Small-Scale Research](https://researchnotes.philipmorganconsulting.com/philip-morgan-research-notes/ssr-guide-public-preview/#1-the-potential-of-small-scale-research)
- [2: What Is Research Generally, And Business Research Specifically?](https://researchnotes.philipmorganconsulting.com/philip-morgan-research-notes/ssr-guide-public-preview/#2-what-is-research-generally-and-business-research-specifically)


# The Small-Scale Research Guide



<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 1: The Potential Of Small-Scale Research


@NOTE: Get the examples I have in front of readers early on in the book :)

This was merely the first example I could find of a particular genre that is well-represented on the Internet:

![[vivaldi_2J3S0446jt.png|vivaldi_2J3S0446jt.png]]

You see what's going on here, right? The tweeter is mocking those who confidently espouse opinions on Twitter without institutional backing for those opinions.

This is a simplification of a more complex problem:

1: Deep genuine experts really don't want their expertise undermined by folks farming likes and shares on social media platforms.

2: It is actually possible to mis-use tools we don't have the proper training to master, and that mis-use can lead to harm.

So @deisidiamonia is -- in their own cartoonish like-farming way -- acting in defense of the honor and social position of real expertise. This is a big part of the cultural backdrop we consider when we think about small-scale research.

If I buy a $5000 vintage Martin guitar to play 3-chord songs on the weekend, nobody's really been harmed by my usage of this tool, it's just that the tool's potential is utterly wasted on my lack of ability to play it well. On the other hand, if my wife complains of abdominal pain and I try to remove her appendix with a $5 X-acto knife, I'm mis-using a tool in a way that almost certainly *will* harm her.

But what if I create a survey instrument, field it and get a few hundred data points, and start advising clients based on what I've learned from that survey? Am I mis-using that tool in a way that can lead to harm?

When we consider doing research, too many of us fear something like the tweet I included above being aimed at us. Either a well-trained master of the tool spots an error that was invisible to us and calls us out, a social media like-farmer takes a pot shot at us, or we cause actual harm to another despite our intentions to help. These fears are not entirely baseless, but they are based on a misapprehension of the world of research that I hope to correct in this guide.

I hope to illuminate a small corner of the much larger world of research: the small-scale research corner. The value of using small-scale research to help your clients make better decisions is high, and -- if you're careful with your usage of the tools -- the risk of causing harm is low.

## Data Is Powerful

Our culture worships data. And rightly so. Data combined with human ingenuity and sweat is a godlike tool that's pulled us out of a nasty, brutish, and short existence into across-the-board increases in comfort, wealth, health, and technological & human potential. But cults form around deities, and so data is more than just a powerful tool.

Data can also be a way to justify taking a quick shortcut from an inner emotional sense to a haughty, external certainty. We can go further and use data as a social cudgel to attack enemies. Or we can assemble enough data to feel that we walk about in priestly vestments, closer to the divine than the unwashed masses.

Data is powerful. But data is not an unalloyed good, nor is it always the best tool to guide decisions. Data can only be as good as those who produce and consume it. But data _can be_ an instrument for improving decision making and wellbeing, and an ability to produce and consume it should be accessible to us, not just large well-funded institutions and companies.  For us to do that, we should start with understanding the broader landscape of research.

## The 5,000-Foot View

We'll roughly divide the world of research into 3 not-equally-sized sectors:

1. Academic/Scientific Research
2. Small-Scale Research
3. Business Research

@TODO: illustrative sketch

**Academic/Scientific research** is what we are most familiar with. Anyone who cites numbers about COVID-19 death rates, case counts, transmissibility, and the like is making use of the output of the academic/scientific research world. If there's one thing that outsiders might know about this world's methods, it's the idea of _statistical validity_. Most of us don't really understand statistical validity, but we know it's important and difficult to achieve, and if we don't like what a given research product seems to say, the easiest way to discredit it is to find some flaw related to statistical validity.

**Small-Scale Research** is something you'll come to understand via this guide. Small-Scale Research (SSR) uses methods that untrained researchers can use without getting wacky results to enable better decision-making within businesses. SSR keeps the cost reasonable by keeping the scope very narrow and using methods that generate insight and contextual richness rather than definitive declarations about cause-effect.

**Business Research** is a superset of SSR that seeks to understand cause-effect in the context of a business decision. Business research also seeks to measure the under-measured in order to help manage risk. And finally, there is a branch of business research that uses inexpensive research methods to earn visibility and trust through social signaling but without supporting (or being on the hook for) any specific decision.

The next chapter of this guide will much more fully explain academic/scientific and business research so we can clearly see where SSR fits in between these two much larger worlds.

It's worth thinking about why we should invest in SSR.

## SSR Forces A Literature Review

After you roughly define your SSR question, you will do a brief literature review. If this sounds intimidating or technical, it's actually not. A SSR literature review is like Googling around for stuff, except using specialized search engines (more and more useful and _free_ options are entering this market all the time). It would be unwise to start a SSR project without doing a literature review, because you don't want or need to duplicate prior efforts. So we could say that SSR _forces_ a literature review.

This is a very good thing. Through the lit review, we'll get a crash course in the relevant prior art. If our SSR question is roughly aimed at understanding the value of branding, we'll find that there have been serious academic inquiries into this question. [@TODO: link to a few scite and others with pre-populated queries for this] Perhaps this will cause us to refine, narrow, adjust, or abandon our SSR project. This is good!

In fact, if _all_ that a SSR project did was motivate a few hours of literature review, most of us would dramatically reduce our ignorance about the prior contributions of academic/scientific research to our area of expertise. This may or may not change how we work with clients, but it can't possibly hurt, and it's likely that our expertise will be enriched.

## SSR Forces Us To Seriously Consider Context

Once you commit to a SSR question, any anxiety that your mind contains will gather itself and start saying, "but what about _this_? What if _this_ is connected somehow to the question I'm investigating?" This is good, because this forces your thinking _outward_ from the SSR question to the _context_ surrounding it.

If your question is "does better branding increase sales?", that's a good starting point question! Good! Whatever anxiety resides in your mind will quickly marshal its forces to ask: "what *other stuff* could increase sales? Or _decrease_ sales even if the branding is helping? Or.... or.... or...?" What's happening here is that you are trying to _locate your SSR question within the larger context of anything and everything that could be connected to it_. This is a VERY GOOD THING!

Eventually your investigation of the surrounding context needs to resolve into a refinement of your initial SSR question so that you can settle down into research, but this preceding "anxious phase" is good because it forces you to seriously consider context, and if there's one thing that can make you a better consultant, it's a better grasp of your client's context.

## SSR Can Create Intellectual Property

Intellectual property (IP) is your expertise packaged and made usable without your direct involvement. For us indie experts, IP is generally not something we invest much effort in protecting in a legal sense or worry about being stolen. Most clients would rather pay us to help apply it, and most competitors are too proud or incompetent to bother with stealing or borrowing it. Sure, there are exceptions, but spending money to protect our IP  would be like buying meteorite strike insurance for a car.

SSR can create or enrich IP. "I want to create IP" is not the best motivation for investing in SSR. "I want to understand X better so I can help my clients make better decisions" is a much better motivation, but the SSR that fulfills your desire to more deeply understand X can end up being, or contributing to, valuable IP. (If the speculative nature of all of this puts you off, that may be a sign that your business or thinking is not in a place that's compatible with SSR.)

## SSR Can Contribute To Your Point Of View

Your point of view (POV) is like an "intellectual fingerprint" -- a way you have of seeing things that is *distinctive* in the market. Every person has a fingerprint, and every person has a point of view. But clients do not find every POV interesting or relevant. Points of view with _content_ that clients find useful, challenging, intriguing, or suggestive of a better path forward are the ones that are most interesting or relevant to them.

Your way of seeing the world is informed by _where you stand_. Do you stand firmly rooted in your own belief or experience, or do you stand more rooted in what _data_ tells you? The content of your point of view will be influenced by the context of where you stand.

I periodically run a workshop that helps consultants clarify and sharpen their point of view. With very few exceptions, participants stand rooted in their own experience, but they want their POV to come more from data. Earlier I said our culture worships data. I wasn't exaggerating, and this explains why most of us want our POV to have the power that data can confer.

SSR can enrich your POV with unique data that you have assembled and interpreted, which can combine powerfully with the output of other, complementary, research. 

## Ultimately SSR Helps Our Clients Make Better Decisions

This is really the bottom line here. It's the ultimate reason to invest in SSR. Well-designed SSR can help our clients make better decisions, which -- bit by bit -- enhances the health of the market we serve, which creates more and better opportunity for us. If you're seeing something like a spirit of service being the most powerful motivator for SSR, then you're seeing this thing clearly.

## SSR Calls For Strength And Humility

Data is powerful. It is also a story we tell ourselves about why we decided a certain way.

This is an argument both for getting more fluent at creating and using data, and an argument for humility around the whole idea of data's value. Some suggested reading for you:

"Alchemy" by Rory Sutherland is a fun, worthwhile read here. "How to Measure Anything" by Douglas Hubbard is a much less fun, but equally worthwhile counterbalancing read.

If you're up for it, read these two books back to back. You'll find yourself suspended in a sort of "intellectual hammock", pulled in two opposing directions with respect to the value of *data*. This is the right place from which to think about this stuff.

## Why Don't We Do More Small-Scale Research?

Let me be clear: well-executed small-scale research is very rare. There are good reasons why.

**We mis-aprehend research generally**, and business research specifically. We hear the word "research" and tend to assume that means expensive, complex, technical, inaccessible stuff. Most of us don't know about this little niche of accessible, useful methods that untrained but motivated people like us can use to create unique value, and so we hear the word "research" and mentally expand that to "not for me".

**We lack formal training in SSR methods**. In college, I did one small-scale research project involving surveys and SPSS as part of a senior Political Science thesis project. That was the extent of my schooling's contribution to my understanding of SSR. Maaaybe if we have done UX or product validation work we've had some exposure to SSR-friendly research methods, but the majority of us lack even semi-formal training in SSR methods. I hope this guide helps, but this lack of training partially explains why SSR is rare.

**We are intimidated by the idea of research**. The like-farming tweeter I referenced at the start of this chapter lives in our head, or at least the social threat they represent lives in our mind as a fear of "getting in over our heads". And so many of us are intimidated by the idea of SSR, because we fixate on the _research_ part (and our associated fears) and undervalue how the _small-scale_ part can make SSR usable and valuable for us.

**We see few examples of our peers doing SSR**. This reinforces the notion that it's difficult, complex, and risky. We have some examples of SSR used for marketing, but that's just one of several ways research can be leveraged, and examples where it's used for decision support are less visible to us and therefore more mysterious.

**We practice an unlicensed profession**, and so there's little incentive for us to raise our game beyond what improvisation, gut feel, past experience, "best practices", and a dash of confidence can achieve. Said more cynically, our clients are surprisingly tolerant of really mediocre consulting services, which reduces the incentive for us to level up the quality of our advice using data. Said more positively, often the status quo at a client is so bad that improvisation, gut feel, past experience, "best practices", and a dash of confidence can create a miraculous amount of relative improvement!

All together, these factors cause us to under-utilize research. Again, many of these are good reasons to _not_ invest in SSR. Doing competent but -- let's be honest -- *utterly ordinary work* can be monetized in totally adequate ways. Building a small team, leveraging a bit of luck, and avoiding making any terrible decisions for 10 or 20 years can buy you two really nice houses, a few college educations for kids, a funded retirement account, and quite a few nice vacations and meals in restaurants. All without touching SSR with a ten foot pole. Not bad!

So after accounting for all the reasons to invest in SSR and considering all the reasons we don't, I think your decision will come down to _dissatisfaction_. The folks who are willing to invest in SSR tend to be dissatisfied with the status quo. They have a hunger to advance the state of the art. A hunger that, frankly, I have been unable to fully explain. I have this hunger. Some folks I know who "should" be earning more money have this hunger and invest in SSR anyway, despite the "illogical" nature of the investment. And I know others who are earning way more money than they need to live well, can't explain how exactly the SSR will contribute more revenue, and also have this hunger.

The best I've got for an explanation: it's a hunger to understand more deeply. We just _have_ to gain this deeper understanding. Maybe this is driven by even deeper, more primal motivations for status, power, etc. I don't really know.

I _do_ know that you can stop reading this guide if you're sure you *don't* have this hunger. There are easier, less risky ways to optimize your business to make more money and serve your clients better.

But if you *do* have this hunger, or if doing SSR is part of your job, or if you're merely curious, then read on. I won't waste your time with anything other than the essential concepts, details, and examples you need to understand and execute small-scale research.


</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 2: What Is Research Generally, And Business Research Specifically?

Small-scale research is accelerated, focused learning. 

Research in general is an attempt to understand causation.

Said a bit more eloquently by [Kanjun Qui](https://kanjun.me):

> I misunderstood the nature of research for most of my life, and this prevented me from doing any. I thought significant research came from following the scientific method until novel discoveries popped out. I'd never contributed something new to human knowledge before, so being a researcher—which required replicating this outcome—felt impossibly far out of reach.
> 
> But it turns out the novel discovery is just a side effect. You don't make novel discoveries by trying to make novel discoveries.
> 
> Instead, research is simply a continuation of something we already naturally do: learning. Learning happens when you understand something that _someone else already understands_. Research happens when you understand something that _nobody else understands yet_.

Source: [https://kanjun.me/writing/research-as-understanding](https://kanjun.me/writing/research-as-understanding)

The whole, short piece is well worth a read, but I can't help quoting one more bit:

> Research, I realized, is what happens as a _byproduct_ when you try to understand something and hit the bounds of what humanity currently knows.[2](https://kanjun.me/writing/research-as-understanding#footnote-2) At that point, there's suddenly no one who can tell you the answer.
> 
> If you care enough about the question, you have to figure out how to answer it yourself, and that's when you start running experiments and developing hypotheses. That rote process of science we're taught in school—to start with a question, generate hypotheses, test with experiments, draw conclusions—it's a good tool, but it doesn't capture the most important element: actually wanting to know the answer to the question!

This... *this* is absolutely the core motivation of small-scale research (SSR): *wanting to know the answer to an un-answered question that you care deeply about*.  This is, more generally, the motivation of most other styles of research, but the _caring about_ part is particularly important and personal when it comes to SSR.

The way most of us think about research causes us to worry about caring or caring too much. We worry using terms like "bias" and "motivated reasoning" and a few others. Basically, we worry that caring too much will interfere with our general objectivity and the effectiveness of our research. But when it comes to SSR, I've found that the worst output is produced by people who _care too little_. They're often creating the research as a social signaling tool ("I do research, therefore I have authoritative insight into X."), but they actually have little or no _practical use_ for the insight the research generates. As a result, they can overlook nonsensical or perhaps more subtly flawed results because they have no skin in the game when it comes to how the results will be _applied_ in real world situations, and they lack the _context_ that practitioners have, causing them to not even sense when the results are nonsensical or flawed.

Almost every apparent constraint that comes with the territory of SSR is actually an opportunity in disguise. Small sample sizes are an opportunity to go deeper and gather more nuance. The lack of rigorous statistical controls is an invitation to get dead serious about contextualizing your findings, which helps you avoid self-deception or cluelessly reporting bad data to the world.

I wanted to start explaining the larger world of research with this focus on the most squishy, human part of SSR: _caring_. Caring about the question, and caring about those who can benefit from an answer to the question (or, more likely, reduced uncertainty around the question). This is both the beating heart that powers and guides SSR, and it's also the thing that others may use to beat up on your method and results. There's no way around how both weakness and strength are bound together in this foundational aspect of SSR.

## What Is Research Generally?

In the business context, there are 4 styles of research:

1. Risk Management
2. Innovation
3. Social Signaling
4. Small-Scale Decision Support

## 1: Risk Management Research

I've had to invent terminology here because I haven't found useful terminology for business research, at least not the way I need to organize things for you in this guide. There are functional categories like _market research_, to name one, but those overindex on outcomes/functions and are granular in the wrong way. Anyway!

Risk management research uses research to help manage risk. Yes, that's a circular definition. :) Let's go a bit deeper.

The primary goal of risk management research is to reduce uncertainty or establish probabilities. You could think of this as simply measuring something that's under-measured. The chief apostle of this style of research is Douglas Hubbard, and the most accessible starting point to his point of view is any one of his [talks available on YouTube](https://www.youtube.com/results?search_query=douglas+hubbard) or his book _How to Measure Anything_.

One of Doug's primary points is that complete certainty is not possible and _rarely necessary_ in a business context. As a result, the kind of extreme rigor applied to many academic or scientific research efforts isn't necessary in a business context. In the business context, there is a ton of value in merely _reducing uncertainty_, and many situations will allow for _significant reductions in uncertainty with just a few measurements_. And often those few measurements are easy and cheap to implement, once you have a sense of what you actually need to measure. Here is a good, short, useful article from Doug on this: [https://hubbardresearch.com/two-ways-you-can-use-small-sample-sizes-to-measure-anything/](https://hubbardresearch.com/two-ways-you-can-use-small-sample-sizes-to-measure-anything/)

A few examples will help illustrate the kind of situations where risk management research is a good fit:

- Reducing uncertainty RE: the cost of a potential new government procurement system
- Quantifying the risk of flooding in a mining operation
- Quantifying the potential impact of particular pesticides regulation.
- Understanding the most significant sources of risk in IT security

All of these examples are from a talk Doug gave, and you can find similar examples in his [talks, available on YouTube](https://www.youtube.com/results?search_query=douglas+hubbard).

Risk management research is best suited to environments that function like closed systems, where you are able to control and measure almost every aspect of the system. Big business enterprises are not closed systems, but they try to function like they are. There's a famous saying (attributed to Peter Drucker, I think?): "what can't be measured can't be managed". This expresses the underlying anxiety about _control_ that's often omnipresent in so many modern organizations, and this desire for control pushes the system's organization and function closer to that of a closed system.

Here's a quick summary of the method that Douglas Hubbard recommends for risk management research:

1. Define the decision you're focused on
2. Model the current uncertainty
3. Compute the value of information that could help reduce that uncertainty
4. Measure, keeping in mind that your goal is reduced uncertainty, not complete certainty
5. Optimize the decision, potentially rinse & repeat the previous steps to further reduce uncertainty

Risk management research roughly fits into the _deductive_ category, meaning that you are testing a hypothesis by gathering data, but much more often risk management research is simply measuring something that hasn't been adequately measured. There's often no hypothesis at all; there's simply a question related to a decision and a process of identifying what should be measured, what value that measurement might create, and the work of doing and interpreting the measurement.

The purpose of risk management research is to reduce uncertainty or establish probabilities; quantitative methods are usually the best fit for this purpose. Another one of Doug's oft-repeated points is that expert intuition is frequently wrong, and simple, inexpensive measurements can outperform expert intuition when it comes to accuracy.[^1] His proposed remedy is to move away from the qualitative world of expert intuition and towards the quantitative world of numbers and estimated probabilities.

Some recommended reading if you're interested in learning more about the risk management style of research:

- [How to Measure Anything : Finding the Value of Intangibles in Business](https://amazon.com/dp/B00INUYS2U)
- [The Flaw of Averages: Why We Underestimate Risk in the Face of Uncertainty](https://amazon.com/dp/B0096CT4VY)

## 2: Innovation Research

Innovation research is done to generate new options or cultivate a more nuanced, detailed, "rich" understanding of a system, process, group of people, or person. People invest in innovation research because they believe[^2] that empathy precedes innovation, and the purpose and methods of innovation research are oriented around increasing empathy, or at least equipping us to better understand others if we have an empathetic intent.

You don't have to look very far into the world of innovation research before you find examples of how empathy leads to economically-valuable innovation. Again, "innovation research" is my invented term for several specific modalities, so if you do want to search more on your own, start by looking for "JTBD success stories". More on these specific modalities in a bit.

The canonical example seems to be how the Mars, Incorporated company used Jobs To Be Done research to learn that quite often the "job" that many Snickers customers "hired" the candy bar to do was to serve as a meal when time was tight. Customers seeing the candy bar as less of a candy and more of a portable, calorically-dense meal substitute opened up new options for how Mars could market the product. Some might even say this research helped create a new category.

Innovation research is especially suited to environments that function like open systems -- ones where you are unable to control, measure, or even fully understand the relationships between elements of the system, or the system you’re investigating and other related systems. Innovation research uses an inductive approach, where you proceed from `observation -> pattern recognition -> development of a theory, model, or conclusion`. This approach is also qualitative in nature; it's not seeking numerical precision, but instead it's seeking a rich, nuanced, detailed understanding of a system, process, group of people, or person along with the same kind of nuanced, detailed, deep grasp of the _context_ surrounding that system, process, group, or person.

This blend of inductive, qualitative research into open systems manifests as one of several branded, semi-defined styles of research:

- Jobs To Be Done research
- Customer Development research
- Ethnographic research
- Problem Space research

(There are probably others that I am not aware of that deserve to be on this list.)

Some recommended reading:

- [When Coffee And Kale Compete](https://amazon.com/dp/B07C7HH662)
- [Indi Young's work](https://indiyoung.com/books/)

## 3: Social Signaling Research

Small-scale research is seeking this:

![[What is research generally, and business research specifically 2022-04-14 07.37.18.excalidraw.png|What is research generally, and business research specifically 2022-04-14 07.37.18.excalidraw.png]]

Research that's done for social signaling reasons is seeking this:

![[What is research generally, and business research specifically 2022-04-14 07.41.14.excalidraw.png|What is research generally, and business research specifically 2022-04-14 07.41.14.excalidraw.png]]

I'm intentionally portraying this third category of research in a harsh, unflattering light to accentuate the differences. This portrayal might make it seem like those who produce this research are calculating and mercenary in their motives, or like cheesy Bond villians, they're stroking a moustache and cackling about how they'll use flimsy research to become authorities mwoohahahaha. I'm certain this isn't their actual motive. 

Rather, I think it's merely easy and convenient to embark on research with reasonably generous intentions and, along the way, to fail to connect the research plan with a specific decision and, also along the way, get dazzled by the easier-to-imitate tools of academic/scientific research (large sample sizes and quantitative methods). This produces "junk food" research -- easy to produce and consume but bereft of actual nutrition for the consumer. And, once your livelihood depends on people buying your junk food, it's tempting to distort the benefits of junk food and cover up the harmful aspects.

![[What is research generally, and business research specifically 2022-04-14 08.02.08.excalidraw.png|What is research generally, and business research specifically 2022-04-14 08.02.08.excalidraw.png]]

To make this all unavoidably more complicated, there is sometimes a context where the output of well-designed SSR *is* somewhat un-connected to a specific decision. This blurs what I wish could be a clear, sharp boundary between high-value SSR and low-value social signaling research. 

Two separate SSR initiatives executed by Tom and Neshka are a good illustration. Tom's question was, "what approach(es) to lead generation are effective for consultants?" He found a meaningful correlation between how the consultant was specialized and what approaches to lead generation were most likely to be effective for them. This enables better decision-making, because a consultant can look at Tom's research product, consider how they are specialized, and eliminate lead generation options that are unlikely to work well for them. There's real value in reducing the amount of trial and error needed to arrive at a working solution to a business problem.

Neshka's question was, "how are museums approaching interdisciplinary exhibits, and what's working?" This question is less tightly-coupled to a specific decision, and not because Neshka was trying to avoid being on the hook for that. Rather, her question exists within a _less commoditized context_. 

A simple but accurate way to understand commoditization is: the process of figuring out how to do something. That "something" could be almost anything that promises some form of advantage, such as:

- Manufacturing smartphones at scale
- Transmitting data wirelessly at high speed
- Finding ways to get cheaper and cheaper workers to build X
- Manipulating groups of people to vote or spend money in a certain way
- Or... design and execute excellent interdisciplinary museum exhibits :)

A less commoditized context is one where the thing being figured out is newer and less thoroughly figured out. This means that a question like "What is the single best way to do X?" is unhelpful, because there simply is no one best, ideal, "correct" way to do X. There may never be _one_ best way, but at this early stage of the commoditization process, people are experimenting with many ways to do X. So SSR into a system that is pretty far away from commoditization will tend to be a survey of what experiments people are trying. Organizing or mapping what experiments people are trying has real value, even thought it's not tightly-coupled to a specific business decision.

A wonderful example of this kind of "mapping" work is Matthew Skelton's _[DevOps Topologies](https://web.archive.org/web/20160305153422/http://web.devopstopologies.com/)_ and _[Team Topologies](https://smile.amazon.com/Team-Topologies-Organizing-Business-Technology/dp/1942788819)_ work. There are many ways to integrate a DevOps team into an organization. Matthew's research into this question has not produced a single "best" way to approach it, but instead a pattern library of the common approaches, along with guidance about the approaches to favor and those to avoid.

![](https://i.imgur.com/xys74gF.png)

Source: [https://web.archive.org/web/20160305153422/http://web.devopstopologies.com/](https://web.archive.org/web/20160305153422/http://web.devopstopologies.com/)

This kind of SSR that seeks patterns within an uncommoditized system produces value by reducing unnecessary experimentation and it plays a pro-commoditization role in helping the system converge on a smaller range of more effective solutions. This kind of SSR is also one wrong turn away from the kind of low-value social signaling research that I recommend you avoid. The output of social signaling research gravitates towards one of three forms:

- "State of the industry" reports
- "What your peers are thinking" reports, or with the same method and different questions you can get a "Where the industry is headed" report as a result
- "What those more successful than you say they did to become successful" reports

These kinds of reports are like catnip -- they make some people come unhinged, both on the production and consumption side of the report. I find that:

- On the production side, because they tend to use quantitative methods, the producers tend to overstate the fidelity and value of the research.
- (Remember my comments about the mystical power of data in Chapter 1.) On the consumption side, people can assume that the number-filled tables and bar charts in this report will... somehow!... make their decisions better because those decisions are now "data-driven" or "evidence-backed".

Both of these positions are based on fantastical thinking. I am happy to criticize the work of those with ill intent, but again, I am convinced that most of the people who produce social signaling research have pretty honorable intent, they've just used an approach that sharply diminishes the value of their investment. That's why I won't specifically name any examples here, but I am thinking of a certain research product that reported on the marketing and ops practices of a certain specialized kind of contracting firm. The main finding of this research could be paraphrased thusly: "Firms that grow at a higher rate do certain things more than those that are growing more slowly." Of course, the report shared what those certain things were. The implicit value proposition of this research is this: "do as they do and it will improve your firm's growth rate." A later chapter will describe the bias that prevents a survey of "what are the winners doing?" from producing useful insight.

I often criticize social signaling research as "low value", but it's not zero-value! It has, at a minimum, some amount of entertainment value, and in some cases a modest amount of information value. Moving away from complete ignorance about where my peers think our industry is headed may not help me make better decisions. Indeed it may cause me to make _worse_ decisions if my peers are not very insightful in their predictions and I follow the herd! But even if there's no decision-improving value in that "where my peers think the industry is headed" report, the entertainment value is more properly described as info-tainment, and my peers' not-very-insightful thinking on where the industry is headed could play a part in me thinking more deeply about the questions and ultimately arriving at my own improved perspective on the question. In short, even low-value research can create *some* value for some people, even if that value is a second-order effect of the research.

Those who look with a cleareyed view at all the options available for business research and choose the social signaling style should feel fine about that -- it's their choice after all. My criticism of this approach is based on a belief that it squanders potential. The same good intentions and resources could be more narrowly and usefully focused on something with more niche appeal and, I will repeatedly argue, more *actual* value and impact in the market. But if you're wanting to trade that impact for greater visibility or reach or temporary fame, I certainly understand why you would choose the social signaling approach to SSR.

## 3: Small-Scale Decision Support Research

I've placed this section on SSR at the end of this chapter so the previous sections on the risk management, innovation, and social signaling styles of research can define the "negative space" that SSR fits into. SSR has both similarities and differences with the risk management and innovation styles of research.

Like the risk management style of SSR, the goal is to support a decision, or a class of decisions (ex: "my clients are always wondering what the lowest-risk way to experiment with Metaverse marketing is, so I want to do some SSR to discover the possibilities and rank them by riskiness"). But the risk management approach presumes a certain level of insider access you may not have, and a certain level of familiarity with the system you may not have. So with the kind of SSR I'm defining and advocating here, you are largely an outsider to any given company that you might be studying. You might be an insider to the market you serve, but as an indie consultant, you are never a full insider to any single company in that market, and so you will face constraints on how much access you have to the people and information contained within each company. If you read Doug Hubbard's book _How to Measure Anything_, you'll start to notice that you don't always have the kind of access to people and information that he's describing. This is fine, but we have to compensate by being scrappier when it comes to recruitment (that's the fancy word for inviting people to participate in your research in some way).

With SSR, we are often blending the quantitative focus of risk management research and the qualitative focus of innovation research. This is known as mixed methods research, and it's certainly not something I've invented. Dr. Sam Ladner has an excellent, sort, readable book, _[Mixed Methods: A short guide to applied mixed methods research](https://amazon.com/dp/B07Q3GVMMK)_, that describes this approach. With mixed methods research, the focus is on the relatively closed system of a specific business decision, not the wide-open vista of innovation. But, because you lack the benefits (and drawbacks!) of being an insider, the research approach might look more like an inductive, qualitative, exploratory approach used by innovation research. SSR is often seeking to understand _scale_ (how big or how common is this thing we're studying?) and _causation_ (does X cause Y?) while also understanding _context_, _nuance_, and _variation_.

The next chapter will elaborate more on SSR, so I'll leave you with the recommendation to read Dr. Ladner's book _[Mixed Methods: A short guide to applied mixed methods research](https://amazon.com/dp/B07Q3GVMMK)_.

## The Academic/Scientific Research

The final "style" of research is academic or scientific research. This style is not relevant to us here except where it spins off commoditized practices that happen to be usable at our scale with our constraints, or methodology approaches we can adapt. In general, academic/scientific research emphasizes large sample sizes and/or statistical controls that are out of our reach. We compensate by making sure our research helps us understand _context_, _nuance_, and _variation_. We may fail to achieve earthshaking findings about causation, but we compensate by gathering the deep, nuanced understanding of context and variation that makes us better consultants and thinkers. This is a great tradeoff for a consultant!


[^1]: I think that Douglas Hubbard is not wrong that in some cases, simple, inexpensive measurements can outperform expert intuition. But I am sure we could also find cases where expert intuition outperforms simple, inexpensive measurements! Despite the occasions where it creates additional tension or complexity in the decision-making process, I'd advocate for a fusion of measurements and expert intuition. While SSR does have as its chief aspiration the improvement of the decisions that your clients make, the path to those improved decisions can be a bit messy.

[^2]: I am careful to call the notion that empathy precedes and leads to innovation a _belief_ because I'm pretty sure it's not a fact or natural law. Throughout human history, we have not always had this belief. At various times, we've believed that innovations (new ideas that create relative advantage) come from a multiplicity of deities, a single deity, from dreams, from psychoactive plants, and from elevated social status or intelligence. I'm sure I'm leaving a few working theories off of that list. I happen to think that the the notion that empathy precedes and leads to innovation is a useful and probably-correct belief, but that still doesn't elevate it from belief to fact.

</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 3: Where Does Small-Scale Research Fit Into The Broader Research Landscape?

- Small-Scale Research:
	- Focused on enabling (better) decision making. As you've seen, that incorporates elements of both risk management and innovation research.
	- The motive of service must supersede all other motives.
	- To make it do-able, we sacrifice statistical rigor and embrace small sample size, mixed methods.
	- If necessary, we look at it as a pilot/prototype or an MVP of more robust research we'll do later, or perhaps as the first "sprint" in a long-running iterative process.
	- How SSR differs in intent, results, and methods (cost/complexity/rigor) from the others
- Warnings:
	- A question:
		- Would you want a new vaccine tested by someone who will never use it on themselves?
	- You need skin in the game
		- This motivates context-gathering, which is critical to sanity-checking your findings.
		- The unavoidable downside of skin in the game: Guillaume story: "Are you OK if this invalidates your work's claimed value prop?"
	- You need to connect your research with a decision
		- Otherwise you run the risk of useless "state of the industry" surveys.
		- These have marketing/visibility value, but much less decision-enabling value and less potential to create IP.
		- The rotten low-hanging fruit of SSR
			- Unfortunately, some of the easier-to-execute SSR approaches are useful only for social signaling, and nearly or completely useless for enabling better decision making.
			- Because these approaches are easier to execute, they're more common. And because they're more common, the availability heuristic causes us to think of these approaches as the default for SSR. They might be the default, but they're a bad default.
- Summary
	- Motive: understand causation.
	- 4 contexts
		- Risk management
		- Innovation
		- Hybrid
		- Academic/Scientific
	- We are, coming from a spirit of service, trying to help our clients make better decisions.




</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 4: Research Methods And Design

## Design/Meta-Methods

- **1: Measuring prevalence**
	- ex: NPS (Among those who respond to this survey, how prevalent are the promoters vs. detractors?)
	- ex: We think employee morale is a problem, but don't know how big a problem. Let's measure how many employees are satisfied vs. dissatisfied with their workplace.
	- Tends to reduce down into a dashboard-level number of "great/good/OK/bad/emergency". Tries to abstract complex reality into a number.
- **2: Measuring the under-measured**
	- Similar to measuring prevalence, but worth calling out separately because having this as the goal will trickle down into slightly or very different design/methods
	- ex: what would it cost us if supply chain disruptions shut down our production for a day, week, month, etc?
	- Often will be a _component_ of a larger or more complex analysis or decision. (ex: is the cost and likelihood of that potential supply chain shutdown big enough that we invest in redundancy?)
- **3: Toy version of more complex system: does input X produce/fail to produce output/effect Y?**
	- ex: if this professor who is from Europe behaves in a warm way with one group of students and a cold way with others, will there be a halo effect?
	- ex: if we put this experimental drug into some tissue in a petri dish, will X happen?
	- ex: If we build this [software tool](https://quantum.country) that uses a [certain approach that we believe can help users memorize difficult subjects](https://numinous.productions/ttft/), what will *actually* happen?
	- Another example of a toy experiment design: https://marginalrevolution.com/marginalrevolution/2022/05/systemic-bias-versus-concentrated-bias.html
- **4: Structured observation that seeks to comprehend and make sense of the behavior, nuance, context, and variation within a phenomenon, system, or form of thinking.**
- Overall:
	- The 3 forms of research design (meta-methods) are:
		- Measurement
		- Testing (a hypothesis)
		- Structured overservation -> sense-making/understanding

## Specific Methods (is Tools a better word?)

### TODO

- [ ] #0-task Skip through my collection of research papers, make sure I haven't missed any high-level category of meta-method

"Toy"

- when I was a child: plastic toy locomotive and can of sterno  
- As an adult: better but not perfect RE: correlation/causation  
- But the idea that we can create “toys” for experimentation has stuck with me  
- Toy is not a dismissive term. Just means: cheap, simplified, but usable. Also... you can _play_ with it! Meaning, try lots of things in a low-stakes way to observe what happens and, sometimes, *be surprised* by what happens.

- A survey (har har!) of methods
	- Sampling methods
		- Census
		- Probability sample
		- Convenience sample
	- Measurement methods
		- Surveys
			- Realtime interview
			- Self-administered
		- Interviews
			- Semi-Structured
		- Direct observation
		- Merely *collecting and organizing* extant data (meta-measurement)
		- Analysis of extant quant/qual data
			- Different from the previous because you don't collect the data yourself.
	- Recruitment
		- Reasons people will participate
			- Goodwill/Interest (they want to)
			- Desire to "be heard"
			- They're forced to
			- They're externally incentivized (gift card, etc.)
			- Some surveys are institutionalized to the extent that recruitment is pretty easy (ex: "Best of Taos")
		- Recruitment methods
			- Outbound
				- Email
				- Social/LInkedIn
					- Can be 1:1 or 1:many
					- Gabby's "lead with a low-effort survey" then following up with direct outreach
				- Paid advertising
				- Appeal from a platform (an email list, an institutional partner's platform, etc.)
				- These are convenience sampling methods, though you could apply some limited statistical correction for bias
			- Inbound
				- The survey itself isn't something anybody would seek out, but...
				- You can inline it with existing inbound marketing-ish stuff like popular+relevant articles, email list content, and so on.
				- These are also convenience sampling methods, of course
			- Outsourced
				- Pay-per-completion research companies can recruit for you
				- These companies may have a relevant panel they can put the survey in front of. If they don't they owe it to you to be honest, but since the model usually is pay-per-completion, they should be honest. :)
					- I don't have any particular negative experience with these companies, but there's a lot of garbage out there, so be careful, and we generally are doing research on groups we should be present with anyway, so involving a third part intermediary is often unnecessary. That's why you're probably getting the vibe that I'm a bit down on using outsourced recruitment.
					- You probably don't need a... probability sample :) ... but if you did these options would become more attractive. But again, you probably don't need a probability sample. You just contextualize your findings within the less rigorous convenience sampling methods we tend to use with TEI.
			- For sake of completeness:
				- We'd probably never use these because they require insider status within a semi-closed system, but for completeness...
				- Intercept
					- Census
					- Convenience
	- About survey questions
		- Screener questions
		- Closed questions
			- Likert scale/rating
			- Yes/No, or other forced binary choice
			- Ranking
		- Open questions
			- One word/number
			- Multi-word/sentence/paragraph response
		- Demographics
		- Avoiding bias
			- Difficult to do, but worth trying!
				- Outside eyes on your questions can help
		- 100% of questions delivered every time or conditionals/branching
	- Mixing methods
		- Quant/Qual
			- Surveys can do both (big open-ended questions with paragraph-length answers), but with limitations in terms of qual richness/nuance/interpretation
		- Inductive/Deductive
	- About interviews
		- Range from Indi Young-style listening sessions to facilitated surveys
			- the latter is technically different than an interview, but still there are "interviews" that are almost as prescribed and rote as a survey
		- Agenda and prompts
			- I think this might be the biggest determinant of how an interview unfolds: how detailed is your agenda?
			- Secondarily: how do you prompt interviewees? Just the question? Do you cave under the pressure of silence and start leading them inappropriately?
		- Setting and context
			- Also important is setting and context. How much time is available? How squeezed is the interviewee? How squeezed are YOU?
		- There's not one "best" approach to interviews.
			- Even with lots of mistakes, you'll get nuanced, useful information that helps you build context and add shades of meaning and nuance to quant data.
			- It's hard to recommend a starting point RE: length because uber-short interviews require more skill while longer interviews require stamina and experience.
				- That said, consider asking for 40m interviews, which are less than an hour by a meaningful amount (good for busy people) and long enough for you to screw up and still walk away with something useful.
		- Record or take notes during or after?
			- Potential chilling effect of recording vs. potential distraction of notetaking.
			- I don't have a recipe for you here. Depnds on your preferences and strengths.
			- Neither drawback (the chilling effect or the distraction) is so bad you must avoid it no matter what, and avoiding your personal weakness might more than compensate for the weakness of the choice you've made.
		- Do enough interviews and you will eventually fail spectacularly. Don't worry; it's all in the game, yo. These extreme outliers teach you very little that you can use to improve your technique, so you just dust yourself off and move on.
	- Analysis			
		- The "data soak"
		- Coding
		- Pulling out vivid data points either of a qual or quant nature
		- Seeking patterns
			- Correlations
			- Instructive outliers
			- Another "data soak" to absorb the vibe of the data/patterns/outliers/"odors" that feel interesting or unusual or unexpected
				- In other words, TIME can really help you notice these patterns
	- Some recommendations
		- Most questions are likely to benefit from mixed methods with a convenience opt-in sampling method
		- Don't rule out other approaches, just know that within TEI (again, you might think of this research project as a prototype or first sprint) the mixed method + convenience sampling tends to be most useful and applicable.

## Research Methods Especially Suited To Small-Scale Research


- Surveys
- Semi-structured interviews/listening sessions
- Measuring through data-scraping
- Digital toys
- 


</div></div>



<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 5: Formulating A Good Small-Scale Research Question

- Overlap between client-side uncertainty, client-side importance, and you-side interest
- Scope of the question will need to be quite narrow
- Warning about Halo Effected questions and explanation of how the Halo Effect produces problematic findings
- Bias, generally.
	- Research bias, specifically sampling bias, just _is_ -- deal with it. :cool:
	- But also... understand it!
	- I've already mentioned question bias. There are other forms of bias.
				- Sampling bias
					- WHO responds
					- We think of bias in people, especially racial or gender bias, as a negative thing; something they need to work on to improve. It is. But in small-scale research, sampling bias just IS. It's nearly unavoidable, and the remedy is: awareness and contextualizing your findings within the sampling method and bias.
					- There are lots of forms of sampling bias. It's basically anything having to do with *who within your larger sample population that ends up responding* and how the relationship between the sample (the subset of the larger population) and the population is distorted by who does/doesn't respond.
						- Maybe the sample is really skewed in some way that distorts the findings
						- Maybe only people with a lot of free time actually respond
						- Maybe only venturesome risk-seeking people respond
						- The above are partially mitigated by having skin in the game! Remember the little story I told in the previous talk about that?
				- Measurement bias
					- WHAT and HOW they respond and tell you
					- Question phrasing
					- Answer design
						- Order
						- Phrasing
					- Interview skill/social membership
			- I'm not trying to discourage you with all this info about bias.
				- Remember the "intellectual hammock" I talked about earlier, where we live in the tension between the power of data and the impotence of data. Understanding the insidious ways in which bias can mislead us is important.
				- The academic/scientific context remedy is pursuing sample size + statistical controls. This is where the idea of statistical significance comes from.
				- The small-scale research context remedy is skin in the game + mixed methods. Both of these lead us to properly incorporate *context*, and that helps us compensate for bias in the way that humans have successfully been for millenia.
				- Also: bias feels like a bummer to talk about. :) That's why I put it a bit later in this talk, even though it's a question you should consider immediately as you begin the research design, and should continue considering throughout the whole project.
				- Some people will be biased against biased data. "That's biased data" is a convenient way to say: "I'm not interested in changing (at all) or (in the way you propose)."
- Examples:
	- Climbing South Sister, the false summit
	- Sampling the deer population around my house


</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 6: Designing A Small-Scale Research Project

From TEI Talks outline:

- TEI Talks 12 of 18 - A Small-Scale Research Stack
	- Broad question
	- Lit review
	- Formulate/refine question
	- Other method design stuff
	- De-biasing survey
	- Recruitment approach
	- Interviews (if necessary)
	- Revised survey, also recruiting for interviews
	- Interviews
	- Analysis
		- The "data soak"
	- Publication

</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 7: Executing A Small-Scale Research Project

- Very lightly document the plan (some folks will benefit from this"north star", others not so much, but this seems like the logical place to start -- sort of like Ari's "commander's intent")
- De-biasing options
- Combined measuring+recruiting survey
- What to promise
	- Anonymity or not?
	- Share back prelim results?
	- Time required to participate
	- Promises about results/impact or not?
	- Disclosures about your role, skin in the game, objectivity, etc?
- Appropriate defaults for soliciting+followup, scheduling, and conducting interviews (recorded or not? etc.)
	- LinkedIn connect+recruit message
	- Email recruit message
	- Other means of recruitment: (use your own judgment + contextual appropriateness for these)
- Summary of grounded theory iteration method and further reading:
	- https://delvetool.com/groundedtheory
	- https://fulcra.design/a-brief-informal-guide-to-doing-grounded-theory
	- https://axle.design/an-integrated-qualitative-analysis-environment-with-obsidian
- What is "enough"/saturation?
- Troubleshooting

</div></div>


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

# 8: Making Small-Scale Research Findings Useful & Impactful

Here is a delightful writeup of some small-scale research that I'm going to pull some quotes from here: [https://numinous.productions/ttft/](https://numinous.productions/ttft/)

Passages like this show a) enthusiasm for the research -- both the question and the in-progress answer, b) humility about the methods and their efficacy, and c) belief in the value of the work, constraints and all:

> These are preliminary results, and need more investigation. One naturally wonders what would happen if we’d been much more aggressive with the review schedule, setting the initial interval between reviews to (say) 2 months? If users reliably retained information up to that point, then the graph would start very high, and we wouldn’t see the exponential. We need to investigate these and many similar questions to better understand what’s going on with user’s memories.
> 
> Early feedback from users makes us cautiously optimistic that they’re finding the mnemonic medium useful. In May 2019, one of us posted to Twitter a short thread explaining the technical details of how quantum teleportation works. One user of Quantum Country replied to the thread with:

I'll skip over quoting the tweets that this article quotes (you should read the [article](https://numinous.productions/ttft/) though; it's quite good!) and move on to this next quote showing both humility and conviction:

> We don’t yet have a good model of exactly what those people are learning, but it seems plausible they are taking away considerably more than from a conventional essay, or perhaps even from a conventional class.
> 
> Of course, this kind of feedback and these kinds of results should be taken with a grain of salt. The mnemonic medium is in its early days, has many deficiencies, and needs improvement in many ways (of which more soon). It is, however, encouraging to hear that some users already find the medium exceptionally helpful, and suggests developing and testing the medium further. At a minimum, it seems likely the mnemonic medium is genuinely helping people remember. And furthermore it has the exponentially increasing efficiency described above: the more people study, the more benefit they get per minute studied.

And then a bit further down, there's this meta-reflection:

> Before we delve deeper into the mnemonic medium, let’s mention one challenge in the discussion: the inherent difficulty in achieving a good balance between conveying enthusiasm and the kind of arm’s-length skepticism appropriate for evaluation. On the one hand, we would not have built the mnemonic medium if we weren’t excited about the underlying ideas, and wanted to develop those enthusiasms. To explain the mnemonic medium well, we need to bring you, the reader, inside that thinking. But having done that, we also need to step back and think more skeptically about questions such as: is this medium really working? What effect is it actually having on people? Can it be made 10x better? 100x better? Or, contrariwise, are there blockers that make this an irredeemably bad or at best mediocre idea? How important a role does memory play in cognition, anyway? So far, we’ve focused on the enthusiastic case for the medium, why one might consider this design at all. But later in this essay we’ll gradually step back and reflect in a more skeptical frame.

This is a beautiful example of how you can write about the findings of small scale research in a way that accentuates the value *and* acknowledges whatever limitations are present. You can do *both*, and it does not reduce the value or importance of the work.

This nuanced and honest approach to presenting findings will be unappealing to those in the audience who are unmotivated to gain a nuanced and honest understanding of the answers you are pursuing. Those who are seeking a simple headline won't be satisfied. That may end up being a lot of people! But niche appeal is baked into the whole idea of small-scale research, and so it shouldn't come as a surprise that our approach to presenting findings is consistent with the whole project's honest and nuanced approach to seeking answers. Those who are seeking the value that context and nuance can add to their decision-making will find your writeup, or the versions of it that you weave into conversation with them, uniquely valuable.


--- 

Outline

- Leveraging for visibility
	- Examples
		- https://www.foregroundweb.com/photography-website-statistics/
		- https://patternreport.com/
		- https://hingemarketing.com/wp-content/uploads/2020/12/Research-VE-Study-Summary.pdf
	- These tend to be less useful for decision making, and more useful for visibility
- Leveraging for IP
	- IP doesn't have to be closed/secret to be a source of power. (Tobie's POV)
	- Examples
		- https://web.devopstopologies.com
			- https://web.archive.org/web/20210526161122/https://web.devopstopologies.com/
		- https://www.cnpatterns.org/patterns-library
- Leveraging for services
- Leveraging for lead gen

</div></div>



## TODO

- [x] #0-task Next step: reference TEI Talks to sketch out chapter outlines 📅 2022-03-11 ✅ 2022-04-12
- [x] #0-task Second outlining pass through all the chapters 📅 2022-04-15 ✅ 2022-04-17
- [ ] Start brain-dump/writing!

## Notes

- Running examples:
	- If you sampled the prevalence of deer at my house at 5 to 6am and 4 to 6pm, you'd get a hugely sample-biased result that over-counts. You'd be looking at the usual behavior of an open system but at a point in that system's oscillation that is pretty far to one extreme of the system's normal state.
	- If you sampled the prevalence of birds in my wife's bird sanctuary the moment I open the door to take the trash out, you'd hugely under-count, again due to sampling bias. You'd be looking at an open system that's been disturbed by my and sampling right after that disturbance has changed things away from the equilibrium.

---


<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

### My Email List

80% notes and updates on my research into indie consulting and thought leadership; 20% random stuff. I share both process and results. I publish irregularly, up to several times/week. Join up:

<iframe src="https://pmcresearchnotes.substack.com/embed" width="100%" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>

</div></div>



<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">



</div>

### Comments

&nbsp;

<script src="https://utteranc.es/client.js"
        repo="philipmorg/philip-morgan-research-notes"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

&nbsp;

</div></div>
